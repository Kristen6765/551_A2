{"cells":[{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00000-dee089d8-880b-4b7e-b925-e14b2ec489c2","output_cleared":false,"source_hash":"93af4235","execution_millis":0,"execution_start":1605575648766},"source":"import sklearn\nimport matplotlib.pyplot as plt\nimport matplotlib as mlp\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder","execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00001-0772ec0d-d0e6-453d-afd7-53fe16000689","output_cleared":false,"source_hash":"bbbdd2ee","execution_millis":3,"execution_start":1605575650607},"source":"class GradientDescent:\n    \n    \n    def __init__(self, eta=0.01, epochs=50, momentum=0.9,\n                 l2=0.0,\n                 minibatches=1,\n                 n_classes=None,\n                 random_seed=None):\n\n        self.eta = eta            #learning rate\n        self.epochs = epochs\n        self.l2 = l2\n        self.minibatches = minibatches\n        self.n_classes = n_classes\n        self.random_seed = random_seed\n        self.momentum=momentum\n\n\n    def _fit(self, X, y, init_params=True):\n        if init_params:\n            if self.n_classes is None:\n                self.n_classes = np.max(y) + 1\n            self._n_features = X.shape[1]\n\n            self.b_, self.w_ ,self.vw_,self.vb_= self._init_params(\n                weights_shape=(self._n_features, self.n_classes),\n                bias_shape=(self.n_classes,),\n                random_seed=self.random_seed)\n            self.cost_ = []\n\n        y_enc = self._one_hot(y=y, n_labels=self.n_classes, dtype=np.float)\n\n        for i in range(self.epochs):\n            for idx in self._yield_minibatches_idx(\n                    n_batches=self.minibatches,\n                    data_ary=y,\n                    shuffle=True):\n                # givens:\n                # w_ -> n_feat x n_classes\n                # b_  -> n_classes\n\n                # net_input, softmax and diff -> n_samples x n_classes:\n                net = self._net_input(X[idx], self.w_, self.b_)\n                softm = self._softmax(net)\n                diff = softm - y_enc[idx]\n                mse = np.mean(diff, axis=0)\n\n                # gradient -> n_features x n_classes\n                grad = np.dot(X[idx].T, diff)/X[idx].shape[0]\n                \n                # update in opp. direction of the cost gradient\n                self.vw_=self.momentum*self.vw_+(1-self.momentum)*grad\n                #self.vb_=self.momentum*self.vb_+(1-self.momentum)*grad\n                self.w_-=self.eta*self.vw_\n                #self.b_-=self.eta*self.vb_\n                #self.w_ -= (self.eta * grad +\n                            #self.eta * self.l2 * self.w_)\n                #self.b_ -= (self.eta * np.sum(diff, axis=0))\n                #print(\"w\",self.w_)\n                #print(\"b\",self.b_)\n\n            # compute cost of the whole epoch\n            net = self._net_input(X, self.w_, self.b_)\n            softm = self._softmax(net)\n            cross_ent = self._cross_entropy(output=softm, y_target=y_enc)\n            cost = self._cost(cross_ent)\n            self.cost_.append(cost)\n        #print(\"retrurn\",self.w_)\n        return self.w_,self.b_\n\n\n    def _net_input(self, X, W, b):\n        return (X.dot(W) + b)\n\n    def _softmax(self, z):\n        return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n\n    def _cross_entropy(self, output, y_target):\n        return - np.sum(np.log(output) * (y_target), axis=1)\n\n    def _cost(self, cross_entropy):\n        L2_term = self.l2 * np.sum(self.w_ ** 2)\n        cross_entropy = cross_entropy + L2_term\n        return 0.5 * np.mean(cross_entropy)\n\n    \n    def _init_params(self, weights_shape, bias_shape=(1,), dtype='float64',\n                     scale=0.01, random_seed=None):\n        \"\"\"Initialize weight coefficients.\"\"\"\n        if random_seed:\n            np.random.seed(random_seed)\n        w=np.zeros(shape=weights_shape)\n        #w = np.random.normal(loc=0.0, scale=scale, size=weights_shape)\n        b = np.zeros(shape=bias_shape)\n        v_w=np.zeros(shape=weights_shape)\n        v_b=np.zeros(shape=bias_shape)\n        return b.astype(dtype), w.astype(dtype),v_w.astype(dtype),v_b.astype(dtype)\n    \n    def _one_hot(self, y, n_labels, dtype):\n        mat = np.zeros((len(y), n_labels))\n        for i, val in enumerate(y):\n            mat[i, val] = 1\n        return mat.astype(dtype)    \n    \n    def _yield_minibatches_idx(self, n_batches, data_ary, shuffle=True):\n            indices = np.arange(data_ary.shape[0])\n\n            if shuffle:\n                indices = np.random.permutation(indices)\n            if n_batches > 1:\n                remainder = data_ary.shape[0] % n_batches\n\n                if remainder:\n                    minis = np.array_split(indices[:-remainder], n_batches)\n                    minis[-1] = np.concatenate((minis[-1],\n                                                indices[-remainder:]),\n                                               axis=0)\n                else:\n                    minis = np.array_split(indices, n_batches)\n\n            else:\n                minis = (indices,)\n\n            for idx_batch in minis:\n                yield idx_batch\n    \n    def _shuffle_arrays(self, arrays):\n        \"\"\"Shuffle arrays in unison.\"\"\"\n        r = np.random.permutation(len(arrays[0]))\n        return [ary[r] for ary in arrays]\n","execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00002-657f471c-bc5e-4e29-a539-4ddd01a03f3d","output_cleared":false,"source_hash":"612943d3","execution_millis":1,"execution_start":1605575653206},"source":"class SoftmaxRegression():\n    def __init__(self, add_bias=True):\n        self.add_bias = add_bias\n        pass\n        \n\n    def fit(self, X, y, optimizer,init_params):\n        Nt = X.shape[0]\n        if self.add_bias:\n            X = np.column_stack([X,np.ones(Nt)])\n        \n        self.w_,self.b_=optimizer._fit(X=X, y=y, init_params=init_params)\n        self._is_fitted = True\n        return self\n    \n    def _predict(self, X):\n        Nt = X.shape[0]\n        if self.add_bias:\n            X = np.column_stack([X,np.ones(Nt)])\n        probas = self.predict_proba(X)\n        return self._to_classlabels(probas)\n \n    def predict(self, X):\n        if not self._is_fitted:\n            raise AttributeError('Model is not fitted, yet.')\n        return self._predict(X)\n\n    def predict_proba(self, X):\n        \n        net = self._net_input(X, self.w_, self.b_)\n        softm = self._softmax(net)\n        #print(\"soft\",softm,self.w_,self.b_)\n        return softm\n\n    def _net_input(self, X, W, b):\n        return (X.dot(W) + b)\n\n    def _softmax(self, z):\n        return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n\n    def _to_classlabels(self, z):\n        return z.argmax(axis=1)\n","execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00003-5ac23765-f3b2-4952-bab8-521098fd1830","output_cleared":false,"source_hash":"cb1b7371","execution_millis":2682,"execution_start":1605502206572},"source":"\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\n\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.5, random_state=0, shuffle=False)\n\n\noptimizer=GradientDescent(eta=0.01, epochs=100, minibatches=1, random_seed=0,momentum=0.9)\nlr = SoftmaxRegression()\nlr.fit(x_train, y_train,optimizer,True)\n\npredicted = lr.fit(x_train, y_train, optimizer,True).predict(x_test)\nprint(predicted)\nprint(\"ACCURACY\")\n\ncount = 0\nfor index, y in enumerate(y_test):\n    if predicted[index] - y != 0:\n        count += 1   #Misclassification rate\n\nprint(100 - count/len(y_test)*100)\n#print(1-sum(predicted-y_test)/y_test.shape)\n_, axes = plt.subplots(2, 8)\nprint(predicted)\nimages_and_labels = list(zip(digits.images, digits.target))\n#for ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):\n#    ax.set_axis_off()\n#    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n#    ax.set_title('Training: %i' % label)\n\n# depending on training set size , adjust [n_samples//2:]\nimages_and_predictions = list(zip(digits.images[n_samples//2:], predicted))\nfor ax, (image, prediction) in zip(axes[1,:], images_and_predictions[:8]):\n    ax.set_axis_off()\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.set_title('P: %i' % prediction)\n\n\n","execution_count":null,"outputs":[{"name":"stdout","text":"[8 8 4 9 0 8 9 1 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 9 6 7 8 9\n 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 9 1 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9\n 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1\n 7 5 4 4 7 2 8 2 2 9 7 9 5 4 4 9 0 8 9 8 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6\n 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 7 8 2 0\n 1 2 6 3 3 7 3 3 4 6 6 6 9 9 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 7 9\n 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7 9 5 4 1 8 4 9 0 8 9 8\n 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0\n 9 5 9 5 4 1 7 7 7 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 5 9\n 5 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 6 4 0 5 3 6 9 6 5 7 5 4\n 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 5 8 9 8 0 9 2 3 4 5 6 7 8 9 0 1 2 3 6 5 6 7\n 8 9 0 1 2 3 6 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 3 7 8 3\n 0 1 2 6 3 3 7 3 3 6 6 6 6 4 9 1 5 0 9 6 2 8 3 0 0 1 7 6 3 2 1 7 4 6 3 1 3\n 9 1 7 6 8 6 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 0\n 1 2 3 4 5 6 7 8 9 0 9 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9\n 8 9 8 4 9 7 7 3 5 9 0 0 2 2 7 9 2 0 9 2 6 3 3 7 3 3 4 6 6 6 4 9 9 5 0 9 5\n 2 9 2 0 0 9 7 6 3 2 3 7 4 6 3 1 3 9 9 7 6 8 4 3 9 4 0 5 3 6 9 6 9 7 9 4 4\n 7 2 5 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 1 2 3 4 5 1 8 1 9 0 1 2 3 4 5 6 9 0\n 4 2 3 4 5 1 7 5 9 4 9 5 5 6 5 0 8 8 5 8 4 1 7 7 3 5 1 6 0 2 2 7 8 2 0 1 2\n 6 1 7 7 7 8 4 6 6 6 8 9 1 5 0 9 5 3 8 0 1 7 6 3 2 1 7 8 6 3 1 3 9 1 7 6 8\n 4 3 1 4 0 5 3 6 3 6 1 7 5 4 4 7 2 2 5 7 3 5 8 4 5 0 8 9 8 0 1 2 3 4 5 6 7\n 8 9 0 1 2 8 4 5 6 7 8 9 0 1 2 5 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7\n 7 5 1 0 0 2 2 7 8 2 0 1 2 6 8 8 7 5 8 4 6 6 6 4 9 1 5 0 9 5 2 8 2 0 0 4 7\n 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 5 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7\n 9 5 4 8 8 4 9 0 8 9 8]\nACCURACY\n92.54727474972191\n[8 8 4 9 0 8 9 1 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 9 6 7 8 9\n 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 9 1 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9\n 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1\n 7 5 4 4 7 2 8 2 2 9 7 9 5 4 4 9 0 8 9 8 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6\n 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 7 8 2 0\n 1 2 6 3 3 7 3 3 4 6 6 6 9 9 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 7 9\n 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7 9 5 4 1 8 4 9 0 8 9 8\n 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0\n 9 5 9 5 4 1 7 7 7 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 5 9\n 5 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 6 4 0 5 3 6 9 6 5 7 5 4\n 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 5 8 9 8 0 9 2 3 4 5 6 7 8 9 0 1 2 3 6 5 6 7\n 8 9 0 1 2 3 6 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 3 7 8 3\n 0 1 2 6 3 3 7 3 3 6 6 6 6 4 9 1 5 0 9 6 2 8 3 0 0 1 7 6 3 2 1 7 4 6 3 1 3\n 9 1 7 6 8 6 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 0\n 1 2 3 4 5 6 7 8 9 0 9 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9\n 8 9 8 4 9 7 7 3 5 9 0 0 2 2 7 9 2 0 9 2 6 3 3 7 3 3 4 6 6 6 4 9 9 5 0 9 5\n 2 9 2 0 0 9 7 6 3 2 3 7 4 6 3 1 3 9 9 7 6 8 4 3 9 4 0 5 3 6 9 6 9 7 9 4 4\n 7 2 5 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 1 2 3 4 5 1 8 1 9 0 1 2 3 4 5 6 9 0\n 4 2 3 4 5 1 7 5 9 4 9 5 5 6 5 0 8 8 5 8 4 1 7 7 3 5 1 6 0 2 2 7 8 2 0 1 2\n 6 1 7 7 7 8 4 6 6 6 8 9 1 5 0 9 5 3 8 0 1 7 6 3 2 1 7 8 6 3 1 3 9 1 7 6 8\n 4 3 1 4 0 5 3 6 3 6 1 7 5 4 4 7 2 2 5 7 3 5 8 4 5 0 8 9 8 0 1 2 3 4 5 6 7\n 8 9 0 1 2 8 4 5 6 7 8 9 0 1 2 5 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7\n 7 5 1 0 0 2 2 7 8 2 0 1 2 6 8 8 7 5 8 4 6 6 6 4 9 1 5 0 9 5 2 8 2 0 0 4 7\n 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 5 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7\n 9 5 4 8 8 4 9 0 8 9 8]\n","output_type":"stream"},{"data":{"text/plain":"<Figure size 432x288 with 16 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00004-168464cb-f97b-4ba8-bca5-d5cf4516b9f5","output_cleared":false,"source_hash":"19fcb40d","execution_millis":49,"execution_start":1605502209266},"source":"from sklearn.datasets import fetch_openml\nfrom sklearn import preprocessing\n#get iris dataset from openml\niris = fetch_openml(name='iris')\nprint(iris.data.shape,iris.target.shape)\nle = preprocessing.LabelEncoder()\nn_samples = iris.target.shape[0]\nprint(iris.data.shape,iris.target.shape)\ndata = iris.data\n\n# standardize\nfor i in range(4):\n    data[:,i] = (data[:,i] - data[:,i].mean()) / data[:,i].std()\n\nlabel_names=np.unique(iris.target)\nle.fit(label_names)\nlabel=le.transform(iris.target)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.5, random_state=0, shuffle=True)\n\n\noptimizer=GradientDescent(eta=0.01, epochs=100, minibatches=1, random_seed=0,momentum=0.9)\nlr = SoftmaxRegression()\nlr.fit(x_train, y_train,optimizer,True)\npredicted = lr.fit(x_train, y_train, optimizer,True).predict(x_test)\nprint(predicted)\nprint(\"ACCURACY\")\n\ncount = 0\nfor index, y in enumerate(y_test):\n    if predicted[index] - y != 0:\n        count += 1   #Misclassification rate\n\nprint(100 - count/len(y_test)*100)","execution_count":null,"outputs":[{"name":"stdout","text":"(150, 4) (150,)\n(150, 4) (150,)\n[2 1 0 2 0 2 0 2 2 2 2 2 2 2 2 0 2 1 0 0 2 2 0 0 2 0 0 2 1 0 2 2 0 2 2 1 0\n 2 2 1 2 0 2 0 0 1 2 2 1 2 1 2 2 1 2 2 1 2 2 2 2 0 2 1 1 1 2 2 0 0 2 2 0 0\n 2]\nACCURACY\n72.0\n/opt/venv/lib/python3.7/site-packages/sklearn/datasets/_openml.py:376: UserWarning: Multiple active versions of the dataset matching the name iris exist. Versions may be fundamentally different, returning version 1.\n  \" {version}.\".format(name=name, version=res[0]['version']))\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00005-b5ad9bf3-fd03-4c4d-a7d9-577cb93f9e7e","output_cleared":false,"source_hash":"6c990d9c","execution_millis":4,"execution_start":1605502209318},"source":"print(label.shape)\nprint(y_test)","execution_count":null,"outputs":[{"name":"stdout","text":"(150,)\n[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n 1 1 1 2 0 2 0 0 1 2 2 2 2 1 2 1 1 2 2 2 2 1 2 1 0 2 1 1 1 1 2 0 0 2 1 0 0\n 1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# HYPERPARAMETER OPTIMIZATION\n\nImplement and use 5-fold cross validation to estimate performance, in terms of accuracy, in\nall of the experiments with respect to batch sizes, learning rates, and momentum parameters, and report the best\nhyper-parameters you found.\n\n\nYou should track and analyze the validation and training accuracy (and optionally\nthe cost) to better understand and analyze the effect of different hyper-parameters. Each fold in cross-validation\nwill produce one of these training and validation curves. Plot one of these training and validation curves for\nseveral representative choices of hyper-parameters in your report.\n\n\"\"\"\nYes, this sentence is confusing. To clarify: after you found out the optimal hyperparameters set, you can alter one hyperparameter at a time and plot the averaged cross validation training and validation accuracy/error. Then analyze the changes of performance on the hyperparameter you altered. \n\"\"\"","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00006-068cb34b-cce1-4c75-9549-d98ec8158a03"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00004-19523848-8a9c-4d36-8118-263bfdd1991f","output_cleared":false,"source_hash":"70341068","execution_millis":1847,"execution_start":1605576481046},"source":"\n# https://towardsdatascience.com/grid-search-in-python-from-scratch-hyperparameter-tuning-3cca8443727b\n\nfrom random import seed\nfrom random import randrange\nimport numpy as np\n\n \n# Split a dataset into k folds\ndef cross_validation(hyperparams, dataset, targets, folds=5):\n    dataset_split = []\n    targets_split = []\n    fold_size = int(len(dataset) / folds)\n    start = 0\n    for i in range(folds):\n        dataset_split.append(dataset[start:start+fold_size])\n        targets_split.append(targets[start:start+fold_size])\n        start += fold_size\n\n    x_train = []\n    y_train = []\n    x_test = []\n    y_test = []\n    accuracies = []\n    t_acc = []\n    v_acc = []\n\n    for index in range(folds):\n        x_test = np.array(dataset_split[index])\n        y_test = np.array(targets_split[index])\n\n        x_train = dataset_split[:index] + dataset_split[index+1:]\n        x_train = np.array([item for items in x_train for item in items])\n        y_train = targets_split[:index] + targets_split[index+1:]\n        y_train = np.array([item for items in y_train for item in items])\n\n        train_acc, val_acc = do_prediction(hyperparams, x_train, y_train, x_test, y_test)\n        #print(\"Run: \", index+1, \"Train Accuracy: \", train_acc)\n        #print(\"Run: \", index+1, \"Val Accuracy: \", val_acc)\n        t_acc.append(train_acc)\n        v_acc.append(val_acc)\n    \n    return np.mean(t_acc), np.mean(v_acc)\n\n\ndef do_prediction(hyperparams, x_train, y_train, x_test, y_test):\n    optimizer=GradientDescent(eta=hyperparams['eta'], epochs=100, minibatches=hyperparams['minibatches'], \n    momentum=hyperparams['momentum'], random_seed=0)\n    lr = SoftmaxRegression()\n\n    model = lr.fit(x_train, y_train, optimizer,True)\n    training = model.predict(x_train)\n    validation = model.predict(x_test)\n    train_acc = 0\n    val_acc = 0\n\n    for index, y in enumerate(y_train):\n        if training[index] - y == 0:\n            train_acc += 1   #Accuracy\n    for index, y in enumerate(y_test):\n        if validation[index] - y == 0:\n            val_acc += 1   #Accuracy\n\n    train_acc = train_acc/len(y_train)*100\n    val_acc = val_acc/len(y_test)*100\n    return train_acc, val_acc\n \n\n# test cross validation split\n#seed(1)\n\nhyperparams = {\n    'eta': 0.1,\n    'minibatches':80,\n    'momentum':0.9,\n}\n\n\"\"\"\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\n\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\n\ncross_validation(hyperparams, data, digits.target, 5)\n#print(folds[0].shape)\n#print(data.shape)\n\"\"\"\n\n# Do validation for each, leaving one as validation set and others as training set\n    \n# minibatch 60 : (100.0, 92.20055710306409)\n# minibatch 100 : (100.0, 91.42061281337047)\n\n\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn import preprocessing\n#get iris dataset from openml\niris = fetch_openml(name='iris')\n\nle = preprocessing.LabelEncoder()\nn_samples = iris.target.shape[0]\n\ndata = iris.data\n\n# standardize\nfor i in range(4):\n    data[:,i] = (data[:,i] - data[:,i].mean()) / data[:,i].std()\n\nlabel_names=np.unique(iris.target)\nle.fit(label_names)\nlabel=le.transform(iris.target)\n#cross_validation(hyperparams, data, label, 5)\n","execution_count":34,"outputs":[{"name":"stderr","text":"/opt/venv/lib/python3.7/site-packages/sklearn/datasets/_openml.py:376: UserWarning: Multiple active versions of the dataset matching the name iris exist. Versions may be fundamentally different, returning version 1.\n  \" {version}.\".format(name=name, version=res[0]['version']))\n","output_type":"stream"},{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"(98.33333333333334, 93.33333333333334)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Hyperparameters GRID SEARCH","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00007-ef2b16e8-af81-4016-a8da-32a4ecbc5b43"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00007-48a7eb63-a6cc-498e-a536-195f6c77716a","output_cleared":false,"source_hash":"64607c61","execution_millis":431172,"execution_start":1605576710026},"source":"### GRID SEARCH CV\n\"\"\"\n        self.eta = eta            #learning rate\n        self.epochs = epochs\n        self.l2 = l2\n        self.minibatches = minibatches\n        self.n_classes = n_classes\n        self.random_seed = random_seed\n        self.momentum=momentum\n\"\"\"\n\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\n\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\n\n\nhyperparams = {\n    'eta': [0.01, 0.05, 0.1, 0.5],\n    'minibatches':[1, 10, 20, 50],\n    'momentum':[0.9, 0.95, 0.99, 0.999],\n}\n\n#hyperparams = {\n#    'eta': [0.01, 0.05, 0.1],\n#    'minibatches':[1, 20, 50],\n#    'momentum':[0.9, 0.95, 0.99],\n#}\n\n\ndef GridSearch(X, Y):\n    graph_t = []\n    graph_v = []\n    graph_w = []\n    graph_x = []\n    graph_y = []\n\n    for eta in hyperparams['eta']:\n        graph_t_row = []\n        graph_v_row = []\n        graph_w_row = []\n        graph_x_row = []\n        graph_y_row = []\n\n        for minibatches in hyperparams['minibatches']:\n\n            for momentum in hyperparams['momentum']:\n                params = {}\n                params['eta'] = eta\n                params['minibatches'] = minibatches\n                params['momentum'] = momentum\n                t_acc, v_acc = cross_validation(params, X, Y, 5)\n                graph_t_row.append(t_acc)\n                graph_v_row.append(v_acc)\n                graph_w_row.append(eta)\n                graph_x_row.append(minibatches)\n                graph_y_row.append(momentum)\n\n        graph_t.append(graph_t_row)\n        graph_v.append(graph_v_row)\n        graph_w.append(graph_w_row)\n        graph_x.append(graph_x_row)\n        graph_y.append(graph_y_row)\n        print('')\n\n    graph_t=np.array(graph_t)\n    graph_v=np.array(graph_v)\n    graph_w=np.array(graph_w)\n    graph_x=np.array(graph_x)\n    graph_y=np.array(graph_y)\n\n    max_v = np.max(graph_v)\n    pos_max_v = np.argwhere(graph_v == np.max(graph_v))[0]\n\n    print('Max Accuracy: %.4f' %(max_v))\n    print('Optimum eta: %f' %(graph_w[pos_max_v[0],pos_max_v[1]]))\n    print('Optimum batch size: %f' %(graph_x[pos_max_v[0],pos_max_v[1]]))\n    print('Optimum momentum: %f' %(graph_y[pos_max_v[0],pos_max_v[1]]))\n\n    return graph_t, graph_v, graph_w, graph_x,graph_y\n    \n\nt, v, w, x, y = GridSearch(data, digits.target)","execution_count":38,"outputs":[{"name":"stdout","text":"\n\n\n/opt/venv/lib/python3.7/site-packages/ipykernel_launcher.py:79: RuntimeWarning: divide by zero encountered in log\n/opt/venv/lib/python3.7/site-packages/ipykernel_launcher.py:79: RuntimeWarning: invalid value encountered in multiply\n\nMax Accuracy: 92.9248\nOptimum eta: 0.010000\nOptimum batch size: 10.000000\nOptimum momentum: 0.900000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Plotting Training and Validation curves as Hyperparameters Vary\n\nPlot one of these training and validation curves for several representative choices of hyperparameters\n\n\"\"\" Yes, this sentence is confusing. To clarify: after you found out the optimal hyperparameters set, you can alter one hyperparameter at a time and plot the averaged cross validation training and validation accuracy/error. Then analyze the changes of performance on the hyperparameter you altered. \"\"\"","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00010-2336ae38-af82-46b0-9f87-2fac2b414712"}},{"cell_type":"code","source":"# Plotting Training and Validation curves\n\n# p = value of hyperparameter that's being varied\n# p_name = name of that hyperparameter (eta, minibatches, momentum)\n\ndef plot_train_val(hyperparams, p_name, p, X, Y):\n    p_values = list(range(0.1, 5, 0.1))\n    p_values = [p*value for value in p_values]\n    for param in p_values:\n        hyperparams[p_name] = param\n        t_acc, v_acc = cross_validation(params, X, Y, 5)\n    \n\n","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00010-8fff29a7-a768-4730-855a-9ab1ffd2172e"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Comparsion against another classifier\n#### data sets\n    iris\n    load_digits\n    \n#### Logistic regression\n#### lasso & ridge regression\n hyperparameter: Learning rate (eta), batch size (minibatches), momentum (momentum)\n ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00005-9630f77e-2e58-4530-a330-92bc858552de","output_cleared":false}},{"cell_type":"markdown","source":"#### function for cross-validation on all possible combination of hyperparameteres","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00013-1a42242c-570c-4786-b984-dac4523f788c"}},{"cell_type":"code","source":"def (X, Y):\n    hyperparams = {\n        'eta': [0.01, 0.05, 0.1, 0.5],\n        'minibatches':[1, 10, 20, 50],\n        'momentum':[0.9, 0.95, 0.99, 0.999],\n    }\n\n    f","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00014-c2623622-92b6-417b-9a48-d9c2a266eece"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Iris","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00011-0fc08e54-97b0-44c3-9c9c-e93f7cfa0b65"}},{"cell_type":"markdown","source":"#### Ridge Regression","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00011-a2b91771-e177-45c8-905a-c4ae6a7133aa"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00006-34da03fd-b0d1-4431-8ac9-9956d06cf03c","output_cleared":false,"source_hash":"49130a23","execution_millis":0,"execution_start":1605505590114},"source":"\nfrom sklearn.linear_model import LinearRegression as LR,Ridge,Lasso\nfrom sklearn.datasets import fetch_openml\nfrom sklearn import preprocessing\n#get iris dataset from openml\niris = fetch_openml(name='iris')\nprint(iris.data.shape,iris.target.shape)\nle = preprocessing.LabelEncoder()\nn_samples = iris.target.shape[0]\nprint(iris.data.shape,iris.target.shape)\ndata = iris.data\n\n# standardize\nfor i in range(4):\n    data[:,i] = (data[:,i] - data[:,i].mean()) / data[:,i].std()\n\nlabel_names=np.unique(iris.target)\nle.fit(label_names)\nlabel=le.transform(iris.target)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.5, random_state=0, shuffle=True)\n\n\n# ridge regression\nreg = Ridge(alpha=100).fit(x_train,y_train) \n\n\nyhat = reg.predict(x_test)\nfrom sklearn.metrics import r2_score\naccuracy=r2_score(y_test,yhat) \nprint(accuracy)\n\n\n","execution_count":null,"outputs":[{"name":"stdout","text":"(150, 4) (150,)\n(150, 4) (150,)\nACCURACY\n0.7731602803655285\n/opt/venv/lib/python3.7/site-packages/sklearn/datasets/_openml.py:376: UserWarning: Multiple active versions of the dataset matching the name iris exist. Versions may be fundamentally different, returning version 1.\n  \" {version}.\".format(name=name, version=res[0]['version']))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Ridge regression with cross-validation","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00012-31484a2c-5f8e-4c48-85e4-635a404a877b"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00013-c5e36b09-9799-4784-a4d8-cb1cb9121cc6"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Lasso Regression","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00014-17b8e992-2c82-427e-96d3-02e2dc9523c2"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00009-c7c6eec4-315d-4b96-a751-c53862e0c78e","output_cleared":false,"source_hash":"2e069f29","execution_millis":0,"execution_start":1605505713926},"source":"lasso_ = Lasso(alpha=0.1).fit(x_train,y_train) \n\nyhat = lasso_.predict(x_test)\nfrom sklearn.metrics import r2_score\naccuracy=r2_score(y_test,yhat) \nprint(accuracy)","execution_count":null,"outputs":[{"name":"stdout","text":"0.8684340508117177\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Lasso Regression with cross-validation","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00015-ae6b6512-0ec3-4c05-96eb-dee044e9ca1d"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00016-d00c240f-154d-4a12-8a94-ebd1bd506917"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Logistic Regression","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00019-6b79804e-c2ac-418f-9e88-48e74669112d"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00020-6d930b1e-df5e-487d-aa02-1ccdf8d6a52e","output_cleared":false,"source_hash":"f46a531f","execution_start":1605506649703,"execution_millis":70},"source":"from sklearn import linear_model\n\nlogreg = linear_model.LogisticRegression(C=1e5)\nlogreg.fit(x_train, y_train)\n\nyhat = logreg.predict(x_test)\nfrom sklearn.metrics import r2_score\naccuracy=r2_score(y_test,yhat) \nprint(accuracy)","execution_count":null,"outputs":[{"name":"stdout","text":"0.9554367201426025\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00014-0ee171ac-9d28-4084-86a6-f6474bdea1aa"}},{"cell_type":"markdown","source":"### digits","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00023-f8838200-ebea-42b2-981a-8357131383c3"}},{"cell_type":"markdown","source":"#### Ridge regression","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00024-f6ca7e11-5efb-4e2f-a66e-734a161f6083"}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression as LR,Ridge,Lasso\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\n\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.5, random_state=0, shuffle=False)\n\n\n# ridge regression\nreg = Ridge(alpha=100).fit(x_train,y_train) \n\n\nyhat = reg.predict(x_test)\nfrom sklearn.metrics import r2_score\naccuracy=r2_score(y_test,yhat) \nprint(accuracy)","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00024-63af3f6a-394c-47aa-9f8d-48d8d1c07cf3","output_cleared":false,"source_hash":"b1ed9d86","execution_start":1605576410670,"execution_millis":83},"outputs":[{"name":"stdout","text":"0.4930141686760242\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"#### Ridge regression with cross-validation","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00025-95bb39c1-fbfe-45da-97ca-f8439de39bfc"}},{"cell_type":"code","source":"","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00026-48235d84-48c6-4ea2-b453-1d783d39894d"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Lasso regression","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00028-9ef33575-8faa-4ed6-aab9-ff85f7dd4d61"}},{"cell_type":"code","source":"lasso_ = Lasso(alpha=0.1).fit(x_train,y_train) \n\nyhat = lasso_.predict(x_test)\nfrom sklearn.metrics import r2_score\naccuracy=r2_score(y_test,yhat) \nprint(accuracy)","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00029-f7734011-947e-45df-9696-37df10928be2","output_cleared":false,"source_hash":"2e069f29","execution_start":1605576622261,"execution_millis":24},"outputs":[{"name":"stdout","text":"0.5157866202624848\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"#### Lasso regression with cross-validation","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00032-dbc7c1f4-f691-4433-96fd-6f5ce081a502"}},{"cell_type":"markdown","source":"#### logistic regression","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00027-b4a114a2-25da-421f-bc37-1c8011e20555"}},{"cell_type":"code","source":"from sklearn import linear_model\n\nlogreg = linear_model.LogisticRegression(C=1e5)\nlogreg.fit(x_train, y_train)\n\nyhat = logreg.predict(x_test)\nfrom sklearn.metrics import r2_score\naccuracy=r2_score(y_test,yhat) \nprint(accuracy)","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00031-db7750db-5414-43f3-a4e8-e4ff186b9904","output_cleared":false,"source_hash":"f46a531f","execution_start":1605576665682,"execution_millis":194},"outputs":[{"name":"stdout","text":"0.8234621061917531\n","output_type":"stream"}],"execution_count":37}],"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"deepnote_notebook_id":"4e4ec626-0e39-410e-b874-36c10f60345b","deepnote_execution_queue":[]}}