{"cells":[{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00000-efb29c94-9687-470a-9588-1ed7cd91459e","output_cleared":false,"source_hash":"8076ed86","execution_millis":1,"execution_start":1604776980488},"source":"# Start writing code here...\nimport sklearn\nimport matplotlib.pyplot as plt\nimport matplotlib as mlp\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport scipy\n\n#https://github.com/alitapan/comp-551/blob/master/Assignment1/logistic_regression/logistic_regression.ipynb \n\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00001-8c67cbdd-6714-4eab-9bad-b3272e5223f3","output_cleared":false,"source_hash":"a861dc2e","execution_millis":153,"execution_start":1604777022776},"source":"# SKLEARN DATASET - DIGITS\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\n\n#digits.images = images\n#digits.target = target \n\n# ONE OPENML DATASET\nfrom sklearn.datasets import fetch_openml\nmice = fetch_openml(name='miceprotein', version=4)\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00002-2aa26544-48a9-4674-9e80-992e45b303eb","output_cleared":false,"source_hash":"f532c7f","execution_millis":0,"execution_start":1604777348506},"source":"def oneHotIt(Y):\n    m = Y.shape[0]\n    #Y = Y[:,0]\n    OHX = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n    OHX = np.array(OHX.todense()).T\n    return OHX\n\ndef softmax(z):\n    z -= np.max(z)\n    sm = (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n    return sm\n\ndef getProbsAndPreds(someX):\n    probs = softmax(np.dot(someX,w))\n    preds = np.argmax(probs,axis=1)\n    return probs,preds\n    \ndef getLoss(w,x,y,lam):\n    m = x.shape[0] #First we get the number of training examples\n    y_mat = oneHotIt(y) #Next we convert the integer class coding into a one-hot representation\n    scores = np.dot(x,w) #Then we compute raw class scores given our input and current weights\n    prob = softmax(scores) #Next we perform a softmax on these scores to get their probabilities\n    loss = (-1 / m) * np.sum(y_mat * np.log(prob)) + (lam/2)*np.sum(w*w) #We then find the loss of the probabilities\n    grad = (-1 / m) * np.dot(x.T,(y_mat - prob)) + lam*w #And compute the gradient for that loss\n    return loss,grad\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00004-7f09c475-f60c-4d6f-8254-612aad1f2168","output_cleared":false,"source_hash":"1ad9503a","execution_millis":0,"execution_start":1604782453626},"source":"logistic = lambda z: 1./ (1 + np.exp(-z))\n\ndef cross_entropy(output, y_target):\n    return - np.sum(np.log(output) * (y_target), axis=1)\n\ndef cost(output, y_target):\n    return np.mean(cross_entropy(output, y_target))\n\ndef softmax(z):\n    z -= np.max(z)\n    sm = (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n    return sm\n\ndef gradient(self, x, y, w):\n    N,D = x.shape\n    yh = softmax(np.dot(x, w))    # predictions  size N\n    print(\"yh\", yh)\n    grad = np.dot(x.T, yh - y)/N        # divide by N because cost is mean over N points\n    return grad\n\ndef J_cost(self, x, y, w):\n    N,D = x.shape\n    yh = softmax(np.dot(x, w))    # predictions  size N\n    J_cost = cost(yh, y)\n    return J_cost \n    \n\nclass GradientDescent:\n    def __init__(self, learning_rate=.001, max_iters=1e4, epsilon=1e-8, record_history=False):\n        self.learning_rate = learning_rate\n        self.max_iters = max_iters\n        self.record_history = record_history\n        self.epsilon = epsilon\n        if record_history:\n            self.w_history = []                 #to store the weight history for visualization\n            \n    def run(self, gradient_fn, x, y, w):\n        grad = np.inf\n        t = 1\n        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n            grad = gradient_fn(x, y, w)               # compute the gradient with present weight\n            print(grad)\n            w = w - self.learning_rate * grad         # weight update step\n            if self.record_history:\n                self.w_history.append(w)\n            t += 1\n        return w,t,grad\n    \n\n\nclass LogisticRegression:\n    \n    def __init__(self, add_bias=True, learning_rate=.1, epsilon=1e-4, max_iters=1e5, verbose=False):\n        self.add_bias = add_bias\n        self.learning_rate = learning_rate\n        self.epsilon = epsilon                        #to get the tolerance for the norm of gradients \n        self.max_iters = max_iters                    #maximum number of iteration of gradient descent\n        self.verbose = verbose\n        \n    def fit(self,x, y,optimizer):\n        if x.ndim == 1:\n            x = x[:, None]\n        if self.add_bias:\n            N = x.shape[0]\n            x = np.column_stack([x,np.ones(N)])\n        N,D = x.shape\n        w0 = np.zeros([D, 10])\n        #self.b_ -= (self.eta * np.sum(diff, axis=0))\n        #w0 = np.random.randn(D,10) * 0.001\n        \n        #self.w = optimizer.run(gradient, x, y)    \n        #g = np.inf \n        #t = 0\n        # the code snippet below is for gradient descent\n        self.w,t,g = optimizer.run(self.gradient, x, y, w0) \n        \n        if self.verbose:\n            print(f'terminated after {t} iterations, with norm of the gradient equal to {np.linalg.norm(g)}')\n            print(f'the weight found: {self.w}')\n        return self\n    \n    def predict(self, x):\n        if x.ndim == 1:\n            x = x[:, None]\n        Nt = x.shape[0]\n        if self.add_bias:\n            x = np.column_stack([x,np.ones(Nt)])\n        yh = logistic(np.dot(x,self.w))            #predict output\n        return yh\n\n    def multiclass_predict(self,x):\n        if x.ndim == 1:\n            x = x[:, None]\n        Nt = x.shape[0]\n        if self.add_bias:\n            x = np.column_stack([x,np.ones(Nt)])\n        prediction=softmax(np.dot(x,self.w))\n        #print(\"x\", x.shape)\n        #print(\"w\", self.w.shape)\n        #print(\"prediction\", prediction.shape)\n        predict_arr=[]\n\n        for i in prediction:\n            predict_arr.append(np.argmax(i))\n\n        return predict_arr\n\n    \n\n#LogisticRegression.gradient = gradient             #initialize the gradient method of the LogisticRegression class with gradient function\nLogisticRegression.gradient = J_cost","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00004-8d27e5b7-bb46-4480-bf5b-2678b95a86c1","output_cleared":false,"source_hash":"8abe5e59","execution_millis":902,"execution_start":1604782456305},"source":"n_samples = len(digits.images)\nx = digits.images.reshape((n_samples, -1))\n\nenc = OneHotEncoder()\ny_enc = oneHotIt(digits.target)\ny = digits.target\n#print(y_enc)\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y_enc, test_size=0.5, random_state=0)\n\nmodel = LogisticRegression(verbose=True)\noptimizer = GradientDescent(learning_rate=.01, max_iters=100, record_history=True)\npredicted = model.fit(x_train, y_train, optimizer).multiclass_predict(x_test)\n\n_, axes = plt.subplots(2, 4)\nprint(predicted)\n\n\nimages_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\nfor ax, (image, prediction) in zip(axes[1,:], images_and_predictions[:4]):\n    ax.set_axis_off()\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.set_title('Prediction: %i' % prediction)\n","execution_count":null,"outputs":[{"name":"stdout","text":"2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\n2.302585092994046\nterminated after 100 iterations, with norm of the gradient equal to 2.302585092994046\nthe weight found: [[-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]\n [-2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924 -2.27955924\n  -2.27955924 -2.27955924 -2.27955924 -2.27955924]]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"},{"data":{"text/plain":"<Figure size 432x288 with 8 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00005-e6bcc70c-f4a9-41fe-b353-d051b205db01"},"source":"# https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py\n# https://medium.com/@awjuliani/simple-softmax-in-python-tutorial-d6b4c4ed5c16\n\n\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0) # only difference\n\n\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.5, random_state=0)\n\nlogistic = lambda z: 1./ (1 + np.exp(-z))\nmodel = LogisticRegression(verbose=True, )\noptimizer = GradientDescent(learning_rate=.01, max_iters=1000, record_history=True)\npredicted = model.fit(x_train, y_train, optimizer).predict(x_test)\n\n\n_, axes = plt.subplots(2, 4)\nprint(predicted)\nimages_and_labels = list(zip(digits.images, digits.target))\nfor ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):\n    ax.set_axis_off()\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.set_title('Training: %i' % label)\n\nimages_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\nfor ax, (image, prediction) in zip(axes[1,:], images_and_predictions[:4]):\n    ax.set_axis_off()\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.set_title('Prediction: %i' % prediction)\n","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"90a929a5-3009-4eff-bc2a-c64f017e9103","deepnote_execution_queue":[]}}